<!DOCTYPE html>
<html lang="english">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Hidden Markov Models (HMM) Explained</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Explain what Hiddem Markov Models are and how to calculate the most likely sequence of Markov chain states given a observable variable sequence" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Heuristic Analyst</a></h1>
                <nav><ul>
                    <li><a href="/category/code.html">Code</a></li>
                    <li><a href="/category/fundamentals.html">Fundamentals</a></li>
                    <li class="active"><a href="/category/machine-learning.html">Machine Learning</a></li>
                    <li><a href="/category/quant.html">Quant</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/hidden-markov-models-hmm-explained.html" rel="bookmark"
           title="Permalink to Hidden Markov Models (HMM) Explained">Hidden Markov Models (HMM) Explained</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2022-08-13T12:01:00+02:00">
                Published: Sat 13 August 2022
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/heuristic-analyst.html">Heuristic Analyst</a>
        </address>
<p>In <a href="/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="/tag/code.html">Code</a> <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/quant.html">Quant</a> </p>
</footer><!-- /.post-info -->      <p>This post builds on the previous post about Markov chains. Now I will explain what Hiddem Markov Models are and how to calculate the most likely sequence of Markov chain states given a observable variable sequence.</p>
<p>Content of this post:</p>
<ol>
<li>What is a HMM</li>
<li>HMM explained</li>
<li>Calculate the probability of a state sequence given observable variable states</li>
<li>Finding the most likely sequence with Brute Force</li>
</ol>
<h1>What is a HMM:</h1>
<p>A Hidden Markov Model is a Markov chain where we cannot observe the state of the Markov chain itself but only other, observable variables, which depend on the – hidden – Markov chain.</p>
<h1>HMM Explained:</h1>
<p>I will use the Markov chain of the post where I explain Markov chains:<br>
<img alt="Markov Chains 1" src="/images/2022_08_13_article_2_picture_1.png"><br>
Lets create a Hidden Markov model from this chain by adding observable states of a variable (you can add as many observable variables you like). I will use John’s face as a observable variable. Either he is happy or sad, all because of the current state (cloudy, rainy or sunny) – with a certain probability at each state:<br>
<img alt="Markov Chains 2" src="/images/2022_08_13_article_2_picture_2.png"><br>
Like in the previous post about Markov chains we can also write down the probabilites of each transition from a current to a future state in a transition matrix, and we can write down the probabilites of each observable variable of each state. The observable probabilities are also known as emission probabilities, here written down in the emission matrix:<br>
<img alt="Markov Chains 3" src="/images/2022_08_13_article_2_picture_3.png"><br>
I will say it again: A Hidden Markov model a Markov chain where the current state is not observable. What is observable are other, Markov chain dependent variables, like in this example John’s mood!</p>
<h1>Calculate the probability of a state sequence given observable variable states:</h1>
<p>Lets say we observe John three times today: In the morning he is happy, afternoon he is happy too but in the evening he is sad. Now a question might occur: Which sequence of states did John most likely experience?<br>
<img alt="Markov Chains 4" src="/images/2022_08_13_article_2_picture_4.png"><br>
Because we cannot just read it from the table we need do the following: We need to calculate the probability for each possible state combination and select the one with the maximum probability – generally speaking we will calculate the joined probability of the observable mood sequence Y and the weather sequence X. Lets do it for one specific sequence:<br></p>
<p>If the following notation applies:</p>
<ul>
<li><span class="math">\(Y_0 = \text{sad John}, Y_1 = \text{happy John}, X_0 = \text{cloudy}, X_1 = \text{rainy}, X_2 = \text{sunny}\)</span></li>
</ul>
<p>then the probability of the sequences:</p>
<ul>
<li>John’s mood = happy, happy, sad</li>
<li>Weather = sunny, cloudy, rainy</li>
</ul>
<p>can be written as:</p>
<ul>
<li><span class="math">\(P(Y = Y_1, Y_1, Y_0 | X = X_2, X_0, X_1)\)</span></li>
</ul>
<p>We can break the porbability calculation <span class="math">\(P(Y = Y_1, Y_1, Y_0 | X = X_2, X_0, X_1)\)</span> down:</p>
<ol>
<li>
<p>state:</p>
<ul>
<li>Calculate the stationary probability of sunny ( because no transition occurring – calculate it with left eigenvector) – <span class="math">\(P( X_2 ) \approx 0.24\)</span></li>
<li>Get the probability of being happy given a sunny weather – <span class="math">\(P( Y_1 | X_2 ) = 0.8\)</span></li>
</ul>
</li>
<li>
<p>state:</p>
<ul>
<li>Get the transition probability of sunny to cloudy – <span class="math">\(P( X_0 | X_2 ) = 0.4\)</span></li>
<li>Get the probability of being happy given a cloudy weather – <span class="math">\(P( Y_1 | X_0 ) = 0.15\)</span></li>
</ul>
</li>
<li>
<p>state:</p>
<ul>
<li>Get the transition probability of cloudy to rainy – <span class="math">\(P( X_1 | X_0 ) = 0.6\)</span></li>
<li>Get the probability of being sad given a sunny weather – <span class="math">\(P( Y_0 | X_1 ) = 0.7\)</span></li>
</ul>
</li>
</ol>
<p>Now we can compute the sequence/path by calculating the product:
<span class="math">\(P(Y = Y_1, Y_1, Y_0 | X = X_2, X_0, X_1) = 0.24 \cdot 0.8 \cdot 0.4 \cdot 0.15 \cdot 0.6 \cdot 0.7 = 0.0048384\)</span></p>
<p>Now we need to comupte the probability of each possible weather sequence and select the sequence with the maximum probability to get the most likely weather sequence!</p>
<h1>Finding the most likely sequence with: Brute Force</h1>
<p>I will implement the same transition matrix probabilities and dependent observable probabilities (John’s mood) as above. The observable sequence, where we want to comput the most likely Markov chain state sequence from, is the following: “happy”, “happy”, “sad”</p>
<p>In the code below I copied the code from this previous post about Markov chains to calculate the left eigenvectors which represent the stationary probability distribution of the Markov chain states. We need this equilibrium for the first state, since it is the start of the sequence and does not transition from another state (hence we use the stationary probability of the state).</p>
<h1>Python code – brute force most likely sequence:</h1>
<div class="highlight"><pre><span></span><code><span class="c1">###########################################</span>
<span class="c1"># The first code-part (between &quot;#####...&quot;) is from a previous post to find the stationary probability of states</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="c1"># all states in one list</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cloudy&quot;</span><span class="p">,</span><span class="s2">&quot;rainy&quot;</span><span class="p">,</span> <span class="s2">&quot;sunny&quot;</span><span class="p">]</span>
<span class="c1"># create transition matrix A - same order as in &quot;states&quot;</span>
<span class="c1"># cloudy, rainy, sunny</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># cloudy</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>   <span class="c1"># rainy</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]]</span>   <span class="c1"># sunny</span>

<span class="c1"># convert matrix to numpy matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># calculate eigenvalues and eigenvectors</span>
<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># probability distribution vector should add up to one - same as lambda of eigenvector equation is 1</span>
<span class="c1"># -&gt; divide each vector element by the sum of each vector&#39;s element</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="o">/</span><span class="n">eigenvectors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># transpose eigenvectors matrix to get a vector easily out of the matrix</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c1"># loop through each vector and see if all elements of vector are greater or equal to zero</span>
<span class="c1"># (because there are no negative probabilities)</span>
<span class="c1"># save valid eigenvectors in &quot;equilibriums</span>
<span class="n">equilibriums</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">)):</span>
    <span class="n">isGreaterEqualZero</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
        <span class="k">if</span> <span class="n">eigenvectors</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">isGreaterEqualZero</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">isGreaterEqualZero</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">equilibriums</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
            <span class="n">equilibriums</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>

<span class="c1">###########################################</span>
<span class="c1"># which sequence is observable of the observable variable?</span>
<span class="c1"># This will be the sequence we will compute the most likely Markov chain states sequence from</span>
<span class="n">sequence_of_observable_variable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">]</span>

<span class="c1"># all observable states in one list</span>
<span class="n">observable_states</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">]</span>
<span class="c1"># Create happy/sad face probabilities dependent on the weather</span>
<span class="c1"># cloudy, rainy, sunny</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>  <span class="c1"># happy face</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]</span>    <span class="c1"># sad face</span>

<span class="c1"># standard library for combination/permutation/product of observable states</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="c1"># create permutation list of states list (each combination of the states given length of the sequence n)</span>
<span class="c1"># combination returned in tuples or triples or ... depends on amount of possible states</span>
<span class="n">products</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_of_observable_variable</span><span class="p">)))</span>

<span class="n">maximum_sequence</span> <span class="o">=</span> <span class="p">()</span> <span class="c1"># tuple or triple or ... depends on amount of possible states</span>
<span class="c1"># starting probability - will be compared to and changed to maximum probability during the iterations</span>
<span class="n">maximum_probability</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">product</span> <span class="ow">in</span> <span class="n">products</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">product</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># first Markov chain state probability is did not transition from another state -&gt; stationary probability</span>
            <span class="c1"># I think there might be several eigenvectors/equilibriums but I do not know whether so</span>
            <span class="c1"># and if so, i do not know which one to take to start, so I just take the first eigenvector/equilibrium</span>
            <span class="n">probability</span> <span class="o">=</span> <span class="n">equilibriums</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">states</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">product</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># state probability is transition probability from before</span>
            <span class="n">current_state</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">product</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">previous_state</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">product</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="c1"># multiply with transition probability from step i-1 to i</span>
            <span class="n">probability</span> <span class="o">*=</span> <span class="n">A</span><span class="p">[</span><span class="n">previous_state</span><span class="p">][</span><span class="n">current_state</span><span class="p">]</span>
        <span class="n">probability</span> <span class="o">*=</span> <span class="n">B</span><span class="p">[</span><span class="n">observable_states</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">sequence_of_observable_variable</span><span class="p">[</span><span class="n">i</span><span class="p">])][</span><span class="n">states</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">product</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
    <span class="k">if</span> <span class="n">probability</span> <span class="o">&gt;</span> <span class="n">maximum_probability</span><span class="p">:</span>
        <span class="n">maximum_probability</span> <span class="o">=</span> <span class="n">probability</span>
        <span class="n">maximum_sequence</span> <span class="o">=</span> <span class="n">product</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximizing sequence:&quot;</span><span class="p">,</span> <span class="n">maximum_sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence&#39;s probability:&quot;</span><span class="p">,</span> <span class="n">maximum_probability</span><span class="p">)</span>
</code></pre></div>

<p><br></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Output</span>
<span class="n">Maximizing</span> <span class="n">sequence</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;rainy&#39;</span><span class="p">)</span>
<span class="n">Sequence</span><span class="s1">&#39;s probability: 0.008575214723926384</span>
</code></pre></div>

<p>We could also easily change the sequence to e.g. <em>“sad”, “happy”, “sad”, “sad”, “happy”, “sad”, “happy”</em> by changing the line 43 in the code:<br></p>
<div class="highlight"><pre><span></span><code><span class="n">sequence_of_observable_variable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sad&quot;</span><span class="p">,</span> <span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">,</span> <span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">,</span> <span class="s2">&quot;happy&quot;</span><span class="p">]</span>
</code></pre></div>

<p><br></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Output</span>
<span class="n">Maximizing</span> <span class="n">sequence</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;rainy&#39;</span><span class="p">,</span> <span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;rainy&#39;</span><span class="p">,</span> <span class="s1">&#39;cloudy&#39;</span><span class="p">,</span> <span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;rainy&#39;</span><span class="p">,</span> <span class="s1">&#39;sunny&#39;</span><span class="p">)</span>
<span class="n">Sequence</span><span class="s1">&#39;s probability: 1.7376287411042944e-05 = 0.000017376287411042944</span>
</code></pre></div>

<p>This was my take on explaining the basis of Hidden Markov Models. Next we will tackle some algorithms relating to HMM’s in order to find the best sequence and other stuff efficiently. Of course the code I used here can be found on my github: <a href="https://github.com/Heuristic-Analyst/heuristic-analyst.com/tree/main/Hidden%20Markov%20Model%20-%20most%20likely%20sequence">www.github.com/Heuristic-Analyst/…</a><br>
Cheers!</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>